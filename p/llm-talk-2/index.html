<!doctype html><html lang=en-us dir=ltr><head><meta charset=utf-8><meta name=viewport content='width=device-width,initial-scale=1'><meta name=description content="从一些工作中扫过。"><title>LLM Talk 2</title>
<link rel=canonical href=https://axi404.github.io/Blog/p/llm-talk-2/><link rel=stylesheet href=/Blog/scss/style.min.0304c6baf04e01a8fe70693791cb744d56a3578a3120a8796cefc66825aa39c7.css><meta property='og:title' content="LLM Talk 2"><meta property='og:description' content="从一些工作中扫过。"><meta property='og:url' content='https://axi404.github.io/Blog/p/llm-talk-2/'><meta property='og:site_name' content='Axi404'><meta property='og:type' content='article'><meta property='article:section' content='Post'><meta property='article:tag' content='AI Talk'><meta property='article:tag' content='LLM'><meta property='article:tag' content='VLM'><meta property='article:tag' content='Embodied AI'><meta property='article:published_time' content='2024-08-16T10:00:00+08:00'><meta property='article:modified_time' content='2024-08-16T10:00:00+08:00'><meta property='og:image' content='https://axi404.github.io/Blog/p/llm-talk-2/cover.jpg'><meta name=twitter:title content="LLM Talk 2"><meta name=twitter:description content="从一些工作中扫过。"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content='https://axi404.github.io/Blog/p/llm-talk-2/cover.jpg'><link rel="shortcut icon" href=/Blog/favicon.png></head><body class=article-page><script>(function(){const e="StackColorScheme";localStorage.getItem(e)||localStorage.setItem(e,"auto")})()</script><script>(function(){const t="StackColorScheme",e=localStorage.getItem(t),n=window.matchMedia("(prefers-color-scheme: dark)").matches===!0;e=="dark"||e==="auto"&&n?document.documentElement.dataset.scheme="dark":document.documentElement.dataset.scheme="light"})()</script><div class="container main-container flex on-phone--column extended"><aside class="sidebar left-sidebar sticky"><button class="hamburger hamburger--spin" type=button id=toggle-menu aria-label="Toggle Menu">
<span class=hamburger-box><span class=hamburger-inner></span></span></button><header><figure class=site-avatar><a href=/Blog/><img src=/Blog/img/avatar_hu12164355173948498683.png width=300 height=300 class=site-logo loading=lazy alt=Avatar>
</a><span class=emoji>❤️</span></figure><div class=site-meta><h1 class=site-name><a href=/Blog>Axi404</a></h1><h2 class=site-description>你好，我是阿汐，欢迎。</h2></div></header><ol class=menu-social><li><a href=https://github.com/Axi404 target=_blank title=GitHub rel=me><svg class="icon icon-tabler icon-tabler-brand-github" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M9 19c-4.3 1.4-4.3-2.5-6-3m12 5v-3.5c0-1 .1-1.4-.5-2 2.8-.3 5.5-1.4 5.5-6a4.6 4.6.0 00-1.3-3.2 4.2 4.2.0 00-.1-3.2s-1.1-.3-3.5 1.3a12.3 12.3.0 00-6.2.0C6.5 2.8 5.4 3.1 5.4 3.1a4.2 4.2.0 00-.1 3.2A4.6 4.6.0 004 9.5c0 4.6 2.7 5.7 5.5 6-.6.6-.6 1.2-.5 2V21"/></svg></a></li><li><a href=/Blog/index.xml target=_blank title=RSS rel=me><svg class="icon icon-tabler icon-tabler-rss" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="5" cy="19" r="1"/><path d="M4 4a16 16 0 0116 16"/><path d="M4 11a9 9 0 019 9"/></svg></a></li></ol><ol class=menu id=main-menu><li><a href=/Blog/><svg class="icon icon-tabler icon-tabler-home" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><polyline points="5 12 3 12 12 3 21 12 19 12"/><path d="M5 12v7a2 2 0 002 2h10a2 2 0 002-2v-7"/><path d="M9 21v-6a2 2 0 012-2h2a2 2 0 012 2v6"/></svg>
<span>Home</span></a></li><li><a href=/Blog/archives/><svg class="icon icon-tabler icon-tabler-archive" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><rect x="3" y="4" width="18" height="4" rx="2"/><path d="M5 8v10a2 2 0 002 2h10a2 2 0 002-2V8"/><line x1="10" y1="12" x2="14" y2="12"/></svg>
<span>Archives</span></a></li><li><a href=/Blog/search/><svg class="icon icon-tabler icon-tabler-search" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="10" cy="10" r="7"/><line x1="21" y1="21" x2="15" y2="15"/></svg>
<span>Search</span></a></li><li><a href=/Blog/repos/><svg class="icon icon-tabler icon-tabler-link" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M10 14a3.5 3.5.0 005 0l4-4a3.5 3.5.0 00-5-5l-.5.5"/><path d="M14 10a3.5 3.5.0 00-5 0l-4 4a3.5 3.5.0 005 5l.5-.5"/></svg>
<span>Repos</span></a></li><li><a href=/Blog/%E5%8F%8B%E9%93%BE/><svg class="icon icon-tabler icon-tabler-link" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M10 14a3.5 3.5.0 005 0l4-4a3.5 3.5.0 00-5-5l-.5.5"/><path d="M14 10a3.5 3.5.0 00-5 0l-4 4a3.5 3.5.0 005 5l.5-.5"/></svg>
<span>友链</span></a></li><li class=menu-bottom-section><ol class=menu><li id=dark-mode-toggle><svg class="icon icon-tabler icon-tabler-toggle-left" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="8" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg>
<svg class="icon icon-tabler icon-tabler-toggle-right" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="16" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg>
<span>Dark Mode</span></li></ol></li></ol></aside><aside class="sidebar right-sidebar sticky"><section class="widget archives"><div class=widget-icon><svg class="icon icon-tabler icon-tabler-hash" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><line x1="5" y1="9" x2="19" y2="9"/><line x1="5" y1="15" x2="19" y2="15"/><line x1="11" y1="4" x2="7" y2="20"/><line x1="17" y1="4" x2="13" y2="20"/></svg></div><h2 class="widget-title section-title">Table of contents</h2><div class=widget--toc><nav id=TableOfContents><ol><li><a href=#前言>前言</a></li><li><a href=#pointllmhttpsarxivorgpdf230816911><a href=https://arxiv.org/pdf/2308.16911>PointLLM</a></a></li><li><a href=#embodiedgpthttpsarxivorgpdf230515021><a href=https://arxiv.org/pdf/2305.15021>EmbodiedGPT</a></a></li><li><a href=#rt-trajectoryhttpsarxivorgpdf231101977><a href=https://arxiv.org/pdf/2311.01977>RT-Trajectory</a></a></li><li><a href=#im2flow2acthttpsarxivorgpdf240715208><a href=https://arxiv.org/pdf/2407.15208>Im2Flow2Act</a></a></li><li><a href=#llarvahttpsarxivorgpdf240611815><a href=https://arxiv.org/pdf/2406.11815>LLARVA</a></a></li><li><a href=#atmhttpsarxivorgpdf240100025><a href=https://arxiv.org/pdf/2401.00025>ATM</a></a></li><li><a href=#track2acthttpsarxivorgpdf240501527><a href=https://arxiv.org/pdf/2405.01527>Track2Act</a></a></li><li><a href=#extreme-cross-embodimenthttpsarxivorgpdf240219432><a href=https://arxiv.org/pdf/2402.19432>Extreme Cross-Embodiment</a></a></li><li><a href=#ecothttpsarxivorgpdf240708693><a href=https://arxiv.org/pdf/2407.08693>ECoT</a></a></li><li><a href=#voxposerhttpsarxivorgpdf230705973><a href=https://arxiv.org/pdf/2307.05973>VoxPoser</a></a></li><li><a href=#moohttpsarxivorgpdf230300905><a href=https://arxiv.org/pdf/2303.00905>MOO</a></a></li><li><a href=#chatgpt-for-roboticshttpsarxivorgpdf230617582><a href=https://arxiv.org/pdf/2306.17582>ChatGPT for Robotics</a></a></li><li><a href=#pivothttpsarxivorgpdf240207872><a href=https://arxiv.org/pdf/2402.07872>PIVOT</a></a></li><li><a href=#code-as-policieshttpsarxivorgpdf220907753><a href=https://arxiv.org/pdf/2209.07753>Code As Policies</a></a></li><li><a href=#mokahttpsarxivorgpdf240303174><a href=https://arxiv.org/pdf/2403.03174>MOKA</a></a></li></ol></nav></div></section></aside><main class="main full-width"><article class="has-image main-article"><header class=article-header><div class=article-image><a href=/Blog/p/llm-talk-2/><img src=/Blog/p/llm-talk-2/cover_hu3774586370577089481.jpg srcset="/Blog/p/llm-talk-2/cover_hu3774586370577089481.jpg 800w, /Blog/p/llm-talk-2/cover_hu2776491514557666069.jpg 1600w" width=800 height=782 loading=lazy alt="Featured image of post LLM Talk 2"></a></div><div class=article-details><header class=article-category><a href=/Blog/categories/ai-talk/ style=background-color:#0177b8;color:#fff>AI Talk</a></header><div class=article-title-wrapper><h2 class=article-title><a href=/Blog/p/llm-talk-2/>LLM Talk 2</a></h2><h3 class=article-subtitle>从一些工作中扫过。</h3></div><footer class=article-time><div><svg class="icon icon-tabler icon-tabler-calendar-time" width="56" height="56" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M11.795 21H5a2 2 0 01-2-2V7a2 2 0 012-2h12a2 2 0 012 2v4"/><circle cx="18" cy="18" r="4"/><path d="M15 3v4"/><path d="M7 3v4"/><path d="M3 11h16"/><path d="M18 16.496V18l1 1"/></svg>
<time class=article-time--published>Aug 16, 2024</time></div><div><svg class="icon icon-tabler icon-tabler-clock" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><polyline points="12 7 12 12 15 15"/></svg>
<time class=article-time--reading>8 minute read</time></div></footer></div></header><section class=article-content><h2 id=前言>前言</h2><p>在代表性工作之余，也有必要阅读一些其他的内容，这当然也需要总结，所以新开设一个章节，记录在这里。</p><h2 id=pointllmhttpsarxivorgpdf230816911><a class=link href=https://arxiv.org/pdf/2308.16911 target=_blank rel=noopener>PointLLM</a></h2><p><img src=/Blog/p/llm-talk-2/PointLLM.png width=1150 height=335 srcset="/Blog/p/llm-talk-2/PointLLM_hu553825487982873834.png 480w, /Blog/p/llm-talk-2/PointLLM_hu5134396539344614531.png 1024w" loading=lazy alt="The pipeline of PointLLM" class=gallery-image data-flex-grow=343 data-flex-basis=823px></p><p>PointLLM 可以是说十分标志的工作了，属于是中规中矩，但是效果确实很不错。就像是一般的 VLM 一样，但是只不过是将图像的模态输入换成了点云，然后使用 point encoder，总体来说改变并不算多。可以说这篇工作的诞生是符合直觉的，点云模态也可以作为一种语言进行建模。</p><h2 id=embodiedgpthttpsarxivorgpdf230515021><a class=link href=https://arxiv.org/pdf/2305.15021 target=_blank rel=noopener>EmbodiedGPT</a></h2><p><img src=/Blog/p/llm-talk-2/EmbodiedGPT.png width=1268 height=636 srcset="/Blog/p/llm-talk-2/EmbodiedGPT_hu17646664235410338742.png 480w, /Blog/p/llm-talk-2/EmbodiedGPT_hu7795959124199465473.png 1024w" loading=lazy alt="The pipeline of EmbodiedGPT" class=gallery-image data-flex-grow=199 data-flex-basis=478px></p><p>EmbodiedGPT 也是一篇比较符合直觉的工作，但是不是那么的极简。本身是按照 BLIP2 的范式来的，用了一个 Embodied-Former（其实也就是 Q-former）来连接 ViT 和 LLaMA3，来做一个桥梁，之后输出一个 instance information，一个 CNN 处理图像输出一个 global information，两个 concat 一下作为 low-level policy 的输入。</p><p>本身值得说的是，一方面这种设计，为什么不单独通过 embodied-former 直接输出的 instance information 呢？毕竟也是通过了 ViT 的信息编码的，之所以还需要一个 CNN，大概率是这样做了之后发现表征能力不强，所以需要更加显式的提供一些信息。</p><h2 id=rt-trajectoryhttpsarxivorgpdf231101977><a class=link href=https://arxiv.org/pdf/2311.01977 target=_blank rel=noopener>RT-Trajectory</a></h2><p><img src=/Blog/p/llm-talk-2/RT-Trajectory.png width=1069 height=561 srcset="/Blog/p/llm-talk-2/RT-Trajectory_hu13764125952902433046.png 480w, /Blog/p/llm-talk-2/RT-Trajectory_hu16656104565665843010.png 1024w" loading=lazy alt="The pipeline of RT-Trajectory" class=gallery-image data-flex-grow=190 data-flex-basis=457px></p><p>RT-Trajectory 是一个输出 low-level policy 的模型，使用了 RT-1 的框架作为动作的输出，在此之前会输入之前和当前的帧以及一个工作轨迹，这里面动作轨迹通过 R 和 G 两个通道表征了时间顺序以及高度信息，和图像一起输入。因为从文字 prompt 改为了图像（轨迹），所以本质上具有更高的细粒度，性能更好也很正常。</p><h2 id=im2flow2acthttpsarxivorgpdf240715208><a class=link href=https://arxiv.org/pdf/2407.15208 target=_blank rel=noopener>Im2Flow2Act</a></h2><p><img src=/Blog/p/llm-talk-2/Im2Flow2Act.png width=1260 height=467 srcset="/Blog/p/llm-talk-2/Im2Flow2Act_hu5565080518049794872.png 480w, /Blog/p/llm-talk-2/Im2Flow2Act_hu10282474898964292122.png 1024w" loading=lazy alt="The pipeline of Im2Flow2Act" class=gallery-image data-flex-grow=269 data-flex-basis=647px></p><p>Im2Flow2Act 算是一篇比较有意思的工作，本身应该是 ATM 的后续工作，不过因为糟糕的阅读顺序，我其实是先阅读的这一篇。</p><p>因为确实需要的前置知识还是很多的，所以说先暂且形而上学的理解一下这个问题，后续估计需要详细的看一看相关的论文。Im2Flow2Act 的核心思想在于，首先根据任务生成对象流，对象流就具有很高的细粒度了，之后对象流通过模仿学习来获得动作规划。</p><p>这篇工作使用了 Diffusion 里面的动作生成（视频生成）作为流生成的方法。首先先框出来一个物体，在物体上面可以采样若干的关键点，这些点就组成了一个 $H\times W$ 的图片，但是这个图片不是正常的图片，和RT-Trajectory 里面的轨迹图片一样，是通过像素表征了别的信息，这里面就是图像系下的坐标和可见度。那么根据条件输入，就可以生成视频了，而这个视频本质上表征的是这个物体在不同时刻的空间信息。</p><p><img src=/Blog/p/llm-talk-2/Im2Flow2Act-2.png width=1294 height=255 srcset="/Blog/p/llm-talk-2/Im2Flow2Act-2_hu12324251025328306913.png 480w, /Blog/p/llm-talk-2/Im2Flow2Act-2_hu4128093339197168695.png 1024w" loading=lazy alt="Flow generation" class=gallery-image data-flex-grow=507 data-flex-basis=1217px></p><p>流生成了之后，基本上是直接使用模仿学习进行的运动规划，用了 Transformer 去编码当前帧的状态，再用 Transformer 去和任务流做融合，来生成剩余的流，最后交给 Diffusion Policy 去生成动作。</p><p>粗浅的凑一下的话，创新性在于使用生成式的方法生成高细粒度的物体流，显然是优于 RT-Trajectory 的，同时第二阶段的时候使用当前的状态和任务流做融合，有一种 nav 中全局规划和局部规划的意味，但是并不完全。总的来说是一篇 based 轨迹的动作规划的很不错的工作，而且相较于 RT-Trajectory，更有细粒度，而且保证了公平性。</p><h2 id=llarvahttpsarxivorgpdf240611815><a class=link href=https://arxiv.org/pdf/2406.11815 target=_blank rel=noopener>LLARVA</a></h2><p><img src=/Blog/p/llm-talk-2/LLARVA.png width=568 height=484 srcset="/Blog/p/llm-talk-2/LLARVA_hu16478159279018403801.png 480w, /Blog/p/llm-talk-2/LLARVA_hu1999976497384346944.png 1024w" loading=lazy alt="The pipeline of LLARVA" class=gallery-image data-flex-grow=117 data-flex-basis=281px></p><p>LLARVA 相较于之前的工作，可以说也是一个比较符合直觉的工作，使用指令调优（IT）的方法进行训练，也是处理了 OXE 这个数据集。从 Pipeline 也不难看出，LLARVA 是一个比较经典的架构，基本上也是 LLAVA 的框架，训练一个 projection layer 以及后面的 Transformer 做对齐以及模态的融合。</p><p><img src=/Blog/p/llm-talk-2/LLARVA-2.png width=1159 height=134 srcset="/Blog/p/llm-talk-2/LLARVA-2_hu16334294272978712019.png 480w, /Blog/p/llm-talk-2/LLARVA-2_hu171522114758417341.png 1024w" loading=lazy alt="The instruction of LLARVA" class=gallery-image data-flex-grow=864 data-flex-basis=2075px></p><p>其创新点其实有点 World Model 的意思，通过让模型预测将来的视觉轨迹这种更具细粒度的内容，之后输出 Action，这明显是一个更加困难而且包含了更多未来信息的任务，所以效果会更好也是显而易见的。当然，本身 IT 的方法，自然也可以让模型更好地完成任务就是了。</p><h2 id=atmhttpsarxivorgpdf240100025><a class=link href=https://arxiv.org/pdf/2401.00025 target=_blank rel=noopener>ATM</a></h2><p><img src=/Blog/p/llm-talk-2/ATM.png width=1781 height=810 srcset="/Blog/p/llm-talk-2/ATM_hu13827657837283113655.png 480w, /Blog/p/llm-talk-2/ATM_hu10889338598371124634.png 1024w" loading=lazy alt="The pipeline of ATM" class=gallery-image data-flex-grow=219 data-flex-basis=527px></p><p>这篇论文可以说影响力还是很拉满的，对于后续的一些轨迹 based 的工作，比如 Im2Flow2Act，明显是有很大的影响的，本身也是拿了 RSS 的满分，不过因为理解了之前的这些论文，这一篇其实很好理解。</p><img src=ATM-2.png alt="Trajectory Conditional Policy" style="display:block;margin:0 auto;zoom:50%"><p>本身的话，ATM 没有采取像是 Im2Flow2Act 一样的物体轨迹的预测，这也比较好理解，全局的点一方面或许可以具有全局的动作视野，而另一方面，全局的点也会比较好获取一些。本身的方法就是使用点跟踪的技术对图像里的点进行跟踪来生成数据集，然后让一个 track transformer 来预测点的轨迹。接下来就是一个正常的 Trajectory Conditional Policy，本身的实现，论文里也说了，也是使用 cls token 去做全局表征（ViT like），然后用了 track prediction 去作为额外的 condition 进行 fusion。</p><p>从创新点来说，这篇算是开山之作之一了，引入了 Track 作为中间的表征以及条件，并且可以通过数据集的一些生成的技术进行标准的损失计算，因此在监督下训练提升的很好也是意料之中了。一方面增加了更具细粒度的输入，一方面这种细粒度也体现在任务的难度上（hard task），二者共同导致模型的简单易用。</p><h2 id=track2acthttpsarxivorgpdf240501527><a class=link href=https://arxiv.org/pdf/2405.01527 target=_blank rel=noopener>Track2Act</a></h2><p><img src=/Blog/p/llm-talk-2/Track2Act.png width=982 height=378 srcset="/Blog/p/llm-talk-2/Track2Act_hu13478923946785409988.png 480w, /Blog/p/llm-talk-2/Track2Act_hu12237593858482207075.png 1024w" loading=lazy alt="DiT of Track2Act" class=gallery-image data-flex-grow=259 data-flex-basis=623px></p><p>老实说，我并没有感觉到 Track2Act 和 ATM 之间是否真的具有较大的差异，二者的方法实际上是近似的，也就是先预测轨迹，之后将轨迹作为动作生成的条件。首先还是进行点的预测，在这里使用的是 DiT，随机 sample 一些点和轨迹，然后就可以进行生成了，将当前状态、目标以及迭代次数都作为 adaptive conditioning 输入。</p><p><img src=/Blog/p/llm-talk-2/Track2Act-2.png width=1544 height=438 srcset="/Blog/p/llm-talk-2/Track2Act-2_hu9268430916526509800.png 480w, /Blog/p/llm-talk-2/Track2Act-2_hu11420025774295333665.png 1024w" loading=lazy alt="Residual policy of Track2Act" class=gallery-image data-flex-grow=352 data-flex-basis=846px></p><p>有了这些点之后，就不难给出一个刚性变化了，然而刚性变化注定不太靠谱，于是乎加入了一个残差策略，再用另一个模型的预测来修正之前的结果。按照文章的表述，残差控制可以增加准确度并未首创，不过确实是一个纠正偏差的好方法，前面的轨迹生成并求刚性变化，获得一个变化之后加上残差，这本质上其实和 ATM 直接通过一个模型进行 action 的求解是等价的，毕竟刚性变化同样可以用模型来进行表征。</p><h2 id=extreme-cross-embodimenthttpsarxivorgpdf240219432><a class=link href=https://arxiv.org/pdf/2402.19432 target=_blank rel=noopener>Extreme Cross-Embodiment</a></h2><p><img src=/Blog/p/llm-talk-2/Extreme-Cross-Embodiment.png width=1178 height=434 srcset="/Blog/p/llm-talk-2/Extreme-Cross-Embodiment_hu5848427255907965082.png 480w, /Blog/p/llm-talk-2/Extreme-Cross-Embodiment_hu2906568667324405879.png 1024w" loading=lazy alt="The pipeline of Extreme Cross-Embodiment" class=gallery-image data-flex-grow=271 data-flex-basis=651px></p><p>这篇文章的感觉有点野心很大故事丰满但是后继乏力的感觉。基本的故事是说要实现一种跨不同机器人模态的表征学习，但是实际上只是视觉导航以及抓取这两种任务，甚至并不涉及灵巧手，这并不能算十分的跨模态。本身的想法就是说，移动和抓取的本质上都是让相机坐标系发生了坐标系变换，实际上是等价的（虽然其实并不等价，因为机械臂受到物理尺寸限制），所以说可以统一，然后就开始直接训练一个模型，输入是 state 和 goal，之后直接融合，获得两个目标，一个是机械臂的位姿（DiT），一个是距离的预测（MLP），也算是将这两个任务统一了一点。</p><p>之前的任务，绝大多数都在处理单一的机器人下的任务，一般为机械臂，这篇的创新点也就止步于同时使用两种训练数据了。然而或许可以思考这样一个问题，假如说机器人的种类是可以穷尽的，或者说常见机器人的种类是可以穷尽的，一种 BEiTV3-like 的模型结构或许是可能的，直接在 Transformer 中引入 EMOE（Embodied MOE），然后同时使用这些全部的数据。</p><h2 id=ecothttpsarxivorgpdf240708693><a class=link href=https://arxiv.org/pdf/2407.08693 target=_blank rel=noopener>ECoT</a></h2><p><img src=/Blog/p/llm-talk-2/ECoT.png width=1319 height=366 srcset="/Blog/p/llm-talk-2/ECoT_hu443176413743909562.png 480w, /Blog/p/llm-talk-2/ECoT_hu9643119002823615094.png 1024w" loading=lazy alt="The pipeline of ECoT" class=gallery-image data-flex-grow=360 data-flex-basis=864px></p><p>ECoT 这篇文章其实算是中规中矩，就是正常的 CoT，但是加入了 Embodied 的条件，能够 work 也是意料之中，或许其生成 CoT 数据的操作是可以借鉴的吧。</p><h2 id=voxposerhttpsarxivorgpdf230705973><a class=link href=https://arxiv.org/pdf/2307.05973 target=_blank rel=noopener>VoxPoser</a></h2><p><img src=/Blog/p/llm-talk-2/VoxPoser.png width=1161 height=352 srcset="/Blog/p/llm-talk-2/VoxPoser_hu6482975638033301594.png 480w, /Blog/p/llm-talk-2/VoxPoser_hu10047120591277572146.png 1024w" loading=lazy alt="The pipeline of VoxPoser" class=gallery-image data-flex-grow=329 data-flex-basis=791px></p><p>VoxPoser 这一篇其实我不太理解，其本身是通过 LLM 以及 VLM 获取图像以及任务的表征，并且想要输出两张价值图，其中 VLM 是传统的 VLM，类似于开集检测器，可以获得物体的位置，之后 LLM 来去处理这些位置，获得两张价值图，这两张价值图进一步引导模型进行轨迹规划。疑点在于，整个的框架的表征被极大的压缩了，本来丰富的视觉特征被压缩到了必要的物体上，之后被 LLM 处理为了价值图，个人感觉这套体系并不稳定，任何一环出了差错，整体就崩掉了。然而使用价值图作为引导是值得参考的，这为模型的轨迹规划提供了更明确的提示。</p><h2 id=moohttpsarxivorgpdf230300905><a class=link href=https://arxiv.org/pdf/2303.00905 target=_blank rel=noopener>MOO</a></h2><p><img src=/Blog/p/llm-talk-2/MOO.png width=1311 height=331 srcset="/Blog/p/llm-talk-2/MOO_hu1561002853506322742.png 480w, /Blog/p/llm-talk-2/MOO_hu8422180632441360151.png 1024w" loading=lazy alt="The pipeline of MOO" class=gallery-image data-flex-grow=396 data-flex-basis=950px></p><p>MOO 的 pipeline 也很简单，本身甚至可以说设置了一个 hard task，而这都是为了设置一个通用的接口。因为 MOO 本身使用了 RT-1 的架构，所以可以理解为，其本身对于复杂的语言表征能力有限，而且不同的任务中，这些语言的格式可能也不相同。不过这个接口，我个人感觉就是本身就是 RT-1 已经具备的。</p><p>大致的流程就像 pipeline 里面描述的一样，其可以将 Mask 作为一个通道融到图像里面，然后将动词提取出来。一个小的疑惑在于，比如说图中的任务，move 是一个向量，没有语序的话，模型如何理解这种顺序呢？然而这并非这篇论文核心探讨的问题，所以其实也无所谓。</p><h2 id=chatgpt-for-roboticshttpsarxivorgpdf230617582><a class=link href=https://arxiv.org/pdf/2306.17582 target=_blank rel=noopener>ChatGPT for Robotics</a></h2><p>本身可以理解为使用 ChatGPT 去做机器人的一个发散性的思考，同时提出了诸如 PromptCraft 之类的工具。</p><h2 id=pivothttpsarxivorgpdf240207872><a class=link href=https://arxiv.org/pdf/2402.07872 target=_blank rel=noopener>PIVOT</a></h2><p><img src=/Blog/p/llm-talk-2/PIVOT.png width=951 height=564 srcset="/Blog/p/llm-talk-2/PIVOT_hu8877757109724288711.png 480w, /Blog/p/llm-talk-2/PIVOT_hu6274401565301454789.png 1024w" loading=lazy alt="The pipeline of PIVOT" class=gallery-image data-flex-grow=168 data-flex-basis=404px></p><p>这篇文章的思想还是比较有趣的，也算是充分利用的 MLLM 的 VLM 能力。本身的思路其实在于，让大模型在具身智能的任务中进行生成式不太靠谱，但是去做选择题还是可以的。于是可以先随机 sample 一些动作或者轨迹，之后将这些内容 annotate 到图片上（与 CoPa 同理解，VLM 的 V 更具有空间的表征能力），让模型选择，然后一次次的选择即可。</p><h2 id=code-as-policieshttpsarxivorgpdf220907753><a class=link href=https://arxiv.org/pdf/2209.07753 target=_blank rel=noopener>Code As Policies</a></h2><img src=Code-As-Policies.png alt="The pipeline of Code As Policies" style="display:block;margin:0 auto;zoom:50%"><p>这篇文章的思路也很简答，就是可以使用代码来控制机器人，这等于可以让 LLM 与环境进行持续且合理的交互。大模型可以通过调用 API 来获取环境信息，比如说调用视觉 API 来获取物体位置，同时也支持了使用一些比如 for 之类的操作，毕竟代码肯定比一次次的生成式更加有条理。</p><h2 id=mokahttpsarxivorgpdf240303174><a class=link href=https://arxiv.org/pdf/2403.03174 target=_blank rel=noopener>MOKA</a></h2><p><img src=/Blog/p/llm-talk-2/MOKA.png width=1359 height=476 srcset="/Blog/p/llm-talk-2/MOKA_hu1227818927646190066.png 480w, /Blog/p/llm-talk-2/MOKA_hu2902875177303240046.png 1024w" loading=lazy alt="The pipeline of MOKA" class=gallery-image data-flex-grow=285 data-flex-basis=685px></p><p>MOKA 的思路其实本质上和 CoPa 以及 PIVOT 是十分类似的，都是使用 Prompt-based 的 VLM，通过将不同的选择 annotate 到图像上，并且让模型进行选择，从而进行路径的规划。MOKA 等于说是希望通过若干的点标注，让模型学会如何去完成动作。所以流程上也是首先先找到需要操作的物体，然后再采样抓握点以及路径点之类的，最后结束。甚至说虽然 MOKA 里面没有明说，但是实际上其对于抓握点进行 filter，并且通过 filter 获得抓握姿态，这个流程实际上和 CoPa 可以说是一模一样，只是说 MOKA 希望通过路径点来完成动作，而 CoPa 则希望通过向量来完成动作。</p></section><footer class=article-footer><section class=article-tags><a href=/Blog/tags/ai-talk/>AI Talk</a>
<a href=/Blog/tags/llm/>LLM</a>
<a href=/Blog/tags/vlm/>VLM</a>
<a href=/Blog/tags/embodied-ai/>Embodied AI</a></section><section class=article-copyright><svg class="icon icon-tabler icon-tabler-copyright" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><path d="M14.5 9a3.5 4 0 100 6"/></svg>
<span>Licensed under MIT</span></section></footer><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css integrity=sha384-n8MVd4RsNIU0tAv4ct0nTaAbDJwPJzDEaqSD1odI+WdtXRGWt2kTvGFasHpSy3SV crossorigin=anonymous><script src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js integrity=sha384-XjKyOOlGwcjNTAIQHIpgOno0Hl1YQqzUOEleOLALmuqehneUG+vnGctmUb0ZY0l8 crossorigin=anonymous defer></script><script src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous defer></script><script>window.addEventListener("DOMContentLoaded",()=>{renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}],ignoredClasses:["gist"]})})</script></article><aside class=related-content--wrapper><h2 class=section-title>Related content</h2><div class=related-content><div class="flex article-list--tile"><article class=has-image><a href=/Blog/p/llm-talk-1/><div class=article-image><img src=/Blog/p/llm-talk-1/cover.fa06aa6c7b5d239b630906b9878b1859_hu10679657573008431902.jpg width=250 height=150 loading=lazy alt="Featured image of post LLM Talk 1" data-hash="md5-+gaqbHtdI5tjCQa5h4sYWQ=="></div><div class=article-details><h2 class=article-title>LLM Talk 1</h2></div></a></article><article class=has-image><a href=/Blog/p/llm-talk-0/><div class=article-image><img src=/Blog/p/llm-talk-0/cover.14d230888754facdc38bcba7c256a1cc_hu10186549384755557257.jpg width=250 height=150 loading=lazy alt="Featured image of post LLM Talk 0" data-hash="md5-FNIwiIdU+s3Di8unwlahzA=="></div><div class=article-details><h2 class=article-title>LLM Talk 0</h2></div></a></article><article class=has-image><a href=/Blog/p/openvla-%E4%BB%A3%E7%A0%81%E7%AC%94%E8%AE%B0/><div class=article-image><img src=/Blog/p/openvla-%E4%BB%A3%E7%A0%81%E7%AC%94%E8%AE%B0/cover.4965e7595483f24480aa5cf1868566ec_hu1944712103896514444.jpg width=250 height=150 loading=lazy alt="Featured image of post OpenVLA 代码笔记" data-hash="md5-SWXnWVSD8kSAqlzxhoVm7A=="></div><div class=article-details><h2 class=article-title>OpenVLA 代码笔记</h2></div></a></article></div></div></aside><script src=https://giscus.app/client.js data-repo=Axi404/Blog data-repo-id=R_kgDOMN0OzQ data-category=Announcements data-category-id=DIC_kwDOMN0Ozc4CgmNX data-mapping=pathname data-strict=0 data-reactions-enabled=1 data-emit-metadata=0 data-input-position=top data-theme=light data-lang=en crossorigin=anonymous async></script><script>function setGiscusTheme(e){let t=document.querySelector("iframe.giscus-frame");t&&t.contentWindow.postMessage({giscus:{setConfig:{theme:e}}},"https://giscus.app")}(function(){addEventListener("message",t=>{if(event.origin!=="https://giscus.app")return;e()}),window.addEventListener("onColorSchemeChange",e);function e(){setGiscusTheme(document.documentElement.dataset.scheme==="light"?"light":"dark")}})()</script><footer class=site-footer><section class=copyright>&copy;
2024 Axi404</section><section class=powerby>Built with <a href=https://gohugo.io/ target=_blank rel=noopener>Hugo</a><br>Theme <b><a href=https://github.com/CaiJimmy/hugo-theme-stack target=_blank rel=noopener data-version=3.26.0>Stack</a></b> designed by <a href=https://jimmycai.com target=_blank rel=noopener>Jimmy</a></section></footer><div class=pswp tabindex=-1 role=dialog aria-hidden=true><div class=pswp__bg></div><div class=pswp__scroll-wrap><div class=pswp__container><div class=pswp__item></div><div class=pswp__item></div><div class=pswp__item></div></div><div class="pswp__ui pswp__ui--hidden"><div class=pswp__top-bar><div class=pswp__counter></div><button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
<button class="pswp__button pswp__button--share" title=Share></button>
<button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
<button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button><div class=pswp__preloader><div class=pswp__preloader__icn><div class=pswp__preloader__cut><div class=pswp__preloader__donut></div></div></div></div></div><div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap"><div class=pswp__share-tooltip></div></div><button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
</button>
<button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)"></button><div class=pswp__caption><div class=pswp__caption__center></div></div></div></div></div><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js integrity="sha256-ePwmChbbvXbsO02lbM3HoHbSHTHFAeChekF1xKJdleo=" crossorigin=anonymous defer></script><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js integrity="sha256-UKkzOn/w1mBxRmLLGrSeyB4e1xbrp4xylgAWb3M42pU=" crossorigin=anonymous defer></script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.min.css crossorigin=anonymous><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.css crossorigin=anonymous></main></div><script src=https://cdn.jsdelivr.net/npm/node-vibrant@3.1.6/dist/vibrant.min.js integrity="sha256-awcR2jno4kI5X0zL8ex0vi2z+KMkF24hUW8WePSA9HM=" crossorigin=anonymous></script><script type=text/javascript src=/Blog/ts/main.js defer></script><script>(function(){const e=document.createElement("link");e.href="https://fonts.googleapis.com/css2?family=Lato:wght@300;400;700&display=swap",e.type="text/css",e.rel="stylesheet",document.head.appendChild(e)})()</script></body></html>